name: MLFlow and Snakemake Workflow

on:
  schedule:
    - cron: '0 8 * * *'  # Run daily at 8:00 UTC
  push:
    branches: [main]
  pull_request:
    branches: [main]

env:
  MLFLOW_URL: https://80f3-105-155-14-70.ngrok-free.app/
  PUSHGATEWAY_URL: https://c8f3-105-155-14-70.ngrok-free.app

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      # [Previous build job steps remain unchanged]
      - name: Checkout code
        uses: actions/checkout@v3
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'
      - name: Set up virtual environment and install dependencies
        run: |
          python -m venv venv
          source venv/bin/activate
          pip install --upgrade pip
          pip install mlflow prometheus_client jq snakemake pulp==2.4 pulp[cbc] stable-baselines3 gym pandas numpy matplotlib shimmy jupyter nbconvert papermill google-auth google-auth-oauthlib google-api-python-client ipykernel pytest pytest-asyncio pytest-mock respx fastapi uvicorn tensorflow scikit-learn redis psycopg2-binary influxdb-client paho-mqtt statsmodels sqlalchemy httpx
          pip list | grep ipykernel || { echo "Erreur : ipykernel non install√©"; exit 1; }
          pip list | grep google-api-python-client || { echo "Erreur : google-api-python-client non install√©"; exit 1; }
        shell: bash
      - name: Install Jupyter kernel
        run: |
          source venv/bin/activate
          python -m ipykernel install --user --name python3 --display-name "Python 3 (venv)"
          echo "Kernel Jupyter install√© avec succ√®s."
        shell: bash
      - name: Save Google Drive token
        run: |
          echo '{"token": "ya29.a0AZYkNZigxH1DhQugv99ZweJfyZdWkklt1F9Ntn6JM5XFhf0q9Km2kecSEgHEgo1zNgKoRWJOUCkGjuGk96SHJKfTIEOMKEYWvzA1Ko3jTx1PR7i_SHwU8wgq-gZ29dwemUnKwgw2jxU0hp9U8jbFI9ZG6bAI9_3WHQ8VMt0xaCgYKAUQSARISFQHGX2Mi1HZSkTbbdf6FiXHFeKU_eA0175", "refresh_token": "1//09sy1DZcnY3jZCgYIARAAGAkSNwF-L9IrW4JaBOZSELGp4ClKWKCcDHRF0kD-Qpw2hOzkXtW-54-pNI29lU0rMPPI63IfdPtxYaE", "token_uri": "https://oauth2.googleapis.com/token", "client_id": "27688368522-s8f6r2og4ikhm9ngnnkgnvtkq01ahu4u.apps.googleusercontent.com", "client_secret": "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", "scopes": ["https://www.googleapis.com/auth/drive"], "universe_domain": "googleapis.com", "account": "", "expiry": "2025-05-03T23:38:04.625991Z"}' | jq '.' > token.json
          if [ -s token.json ]; then
            echo "token.json cr√©√© avec succ√®s."
            cat token.json | jq . || { echo "Erreur : token.json n'est pas un JSON valide"; exit 1; }
          else
            echo "Erreur : token.json est vide ou n'a pas pu √™tre cr√©√©."
            exit 1
          fi
        shell: bash
      - name: Download notebook and scripts
        run: |
          wget https://raw.githubusercontent.com/elmekadem-narjiss/BackUp_ML/refs/heads/main/Backend/ppo_pipeline.ipynb -O ppo_pipeline.ipynb
          wget https://raw.githubusercontent.com/elmekadem-narjiss/BackUp_ML/refs/heads/main/Backend/train_ppo.py -O train_ppo.py
          wget https://raw.githubusercontent.com/elmekadem-narjiss/BackUp_ML/refs/heads/main/Backend/evaluate_ppo.py -O evaluate_ppo.py
          wget https://raw.githubusercontent.com/elmekadem-narjiss/BackUp_ML/refs/heads/main/Backend/BESSBatteryEnv.py -O BESSBatteryEnv.py
        shell: bash
      - name: Download LSTM predictions from Google Drive
        run: |
          source venv/bin/activate
          python Backend/download_file.py || { echo "√âchec de l'√©tape Download LSTM predictions from Google Drive"; exit 1; }
          if [ -f lstm_predictions_charger.csv ]; then
            echo "Fichier lstm_predictions_charger.csv t√©l√©charg√© avec succ√®s."
            ls -l lstm_predictions_charger.csv
          else
            echo "Erreur : lstm_predictions_charger.csv n'a pas √©t√© cr√©√©."
            exit 1
          fi
        shell: bash
      - name: Execute notebook
        run: |
          source venv/bin/activate
          mkdir -p output
          python -m papermill ppo_pipeline.ipynb output/ppo_pipeline_executed.ipynb \
            -p MLFLOW_URL $MLFLOW_URL \
            -p PUSHGATEWAY_URL $PUSHGATEWAY_URL \
            -p output_dir output \
            -p file_path lstm_predictions_charger.csv \
            --kernel python3
          echo "V√©rification des fichiers JSON g√©n√©r√©s :"
          ls -l output/*.json || echo "Aucun fichier JSON trouv√© dans output/"
        shell: bash
      - name: Check PushGateway accessibility
        run: |
          echo "V√©rification de l'accessibilit√© de la PushGateway : $PUSHGATEWAY_URL"
          echo "Envoi de la m√©trique de test vers $PUSHGATEWAY_URL/metrics/job/mlflow_and_snakemake_metrics"
          STATUS=$(echo "test_accessibility{job=\"mlflow_and_snakemake_metrics\"} 1" | curl -s -L -w "%{http_code}" --data-binary @- $PUSHGATEWAY_URL/metrics/job/mlflow_and_snakemake_metrics -o /dev/null)
          if [ "$STATUS" -eq 200 ]; then
            echo "PushGateway accessible, code HTTP : $STATUS"
            echo "V√©rification de la m√©trique test_accessibility dans PushGateway :"
            curl -s -L $PUSHGATEWAY_URL/metrics | grep test_accessibility || echo "M√©trique test_accessibility non trouv√©e"
          else
            echo "Erreur : PushGateway renvoie le code HTTP $STATUS"
            echo "D√©bogage : En-t√™tes de la requ√™te POST :"
            curl -v -L -X POST $PUSHGATEWAY_URL/metrics/job/mlflow_and_snakemake_metrics 2>&1 || echo "√âchec de la requ√™te POST"
            echo "D√©bogage : En-t√™tes de la requ√™te GET pour l'endpoint metrics :"
            curl -v -L $PUSHGATEWAY_URL/metrics 2>&1 || echo "√âchec de la requ√™te GET"
            exit 1
          fi
        shell: bash
      - name: Push selected MLFlow metrics to Prometheus
        run: |
          source venv/bin/activate
          declare -A metrics_92f5=(
            [loss]=0.04144944250583649
            [mae]=0.1347774024638297
            [mse]=0.039917658308037784
            [r2_score]=0.6619863834818595
            [rmse]=0.19979403972100315
          )
          declare -A metrics_ad0c=(
            [aic]=3157.583271036795
            [bic]=3180.9434357641003
            [df_model]=5
            [df_resid]=Inf
            [llf]=-1573.7916355183975
            [mse]=0.005455112014267468
            [scale]=1
          )
          echo "Envoi des m√©triques metrics_92f5..."
          for key in "${!metrics_92f5[@]}"; do
            value=${metrics_92f5[$key]}
            echo "Envoi : $key{run=\"92f5893e1dbe4175a3f4313bc89c56b4\",job=\"mlflow_and_snakemake_metrics\"} $value"
            STATUS=$(echo "$key{run=\"92f5893e1dbe4175a3f4313bc89c56b4\",job=\"mlflow_and_snakemake_metrics\"} $value" | curl -s -L -w "%{http_code}" --data-binary @- $PUSHGATEWAY_URL/metrics/job/mlflow_and_snakemake_metrics -o /dev/null)
            if [ "$STATUS" -eq 200 ]; then
              echo "Succ√®s : M√©trique $key envoy√©e, code HTTP $STATUS"
            else
              echo "Erreur : √âchec de l'envoi de la m√©trique $key, code HTTP $STATUS"
              exit 1
            fi
          done
          echo "Envoi des m√©triques metrics_ad0c..."
          for key in "${!metrics_ad0c[@]}"; do
            value=${metrics_ad0c[$key]}
            echo "Envoi : $key{run=\"ad0cf78265204f34b84a40aa09895c7f\",job=\"mlflow_and_snakemake_metrics\"} $value"
            STATUS=$(echo "$key{run=\"ad0cf78265204f34b84a40aa09895c7f\",job=\"mlflow_and_snakemake_metrics\"} $value" | curl -s -L -w "%{http_code}" --data-binary @- $PUSHGATEWAY_URL/metrics/job/mlflow_and_snakemake_metrics -o /dev/null)
            if [ "$STATUS" -eq 200 ]; then
              echo "Succ√®s : M√©trique $key envoy√©e, code HTTP $STATUS"
            else
              echo "Erreur : √âchec de l'envoi de la m√©trique $key, code HTTP $STATUS"
              exit 1
            fi
          done
          echo "V√©rification des fichiers PPO JSON :"
          if [ -f output/ppo_bess_model_metrics.json ]; then
            echo "Fichier ppo_bess_model_metrics.json trouv√©. Contenu :"
            cat output/ppo_bess_model_metrics.json | jq . || echo "Erreur : ppo_bess_model_metrics.json n'est pas un JSON valide"
            TRAIN_METRICS=$(cat output/ppo_bess_model_metrics.json | jq -r 'to_entries[] | .key + "{run=\"ppo_training\",job=\"ppo_metrics\"} " + (.value | tostring)')
            if [ -z "$TRAIN_METRICS" ]; then
              echo "Erreur : Aucune m√©trique trouv√©e dans ppo_bess_model_metrics.json"
              exit 1
            else
              while IFS= read -r metric; do
                echo "Envoi de la m√©trique PPO (entra√Ænement) : $metric"
                STATUS=$(echo "$metric" | curl -s -L -w "%{http_code}" --data-binary @- $PUSHGATEWAY_URL/metrics/job/ppo_metrics -o /dev/null)
                if [ "$STATUS" -eq 200 ]; then
                  echo "Succ√®s : M√©trique PPO envoy√©e, code HTTP $STATUS"
                else
                  echo "Erreur : √âchec de l'envoi de la m√©trique PPO, code HTTP $STATUS"
                  exit 1
                fi
              done <<< "$TRAIN_METRICS"
            fi
          else
            echo "Erreur : ppo_bess_model_metrics.json non trouv√©. Aucune m√©trique PPO d'entra√Ænement envoy√©e."
            exit 1
          fi
          if [ -f output/evaluation_metrics.json ]; then
            echo "Fichier evaluation_metrics.json trouv√©. Contenu :"
            cat output/evaluation_metrics.json | jq . || echo "Erreur : evaluation_metrics.json n'est pas un JSON valide"
            EVAL_METRICS=$(cat output/evaluation_metrics.json | jq -r 'to_entries[] | .key + "{run=\"ppo_evaluation\",job=\"ppo_metrics\"} " + (.value | tostring)')
            if [ -z "$EVAL_METRICS" ]; then
              echo "Erreur : Aucune m√©trique trouv√©e dans evaluation_metrics.json"
              exit 1
            else
              while IFS= read -r metric; do
                echo "Envoi de la m√©trique PPO (√©valuation) : $metric"
                STATUS=$(echo "$metric" | curl -s -L -w "%{http_code}" --data-binary @- $PUSHGATEWAY_URL/metrics/job/ppo_metrics -o /dev/null)
                if [ "$STATUS" -eq 200 ]; then
                  echo "Succ√®s : M√©trique PPO envoy√©e, code HTTP $STATUS"
                else
                  echo "Erreur : √âchec de l'envoi de la m√©trique PPO, code HTTP $STATUS"
                  exit 1
                fi
              done <<< "$EVAL_METRICS"
            fi
          else
            echo "Erreur : evaluation_metrics.json non trouv√©. Aucune m√©trique PPO d'√©valuation envoy√©e."
            exit 1
          fi
        shell: bash
      - name: Push global success metric to Prometheus
        run: |
          STATUS=$(echo "success{job=\"mlflow_and_snakemake_metrics\"} 1" | curl -s -L -w "%{http_code}" --data-binary @- $PUSHGATEWAY_URL/metrics/job/mlflow_and_snakemake_metrics -o /dev/null)
          if [ "$STATUS" -eq 200 ]; then
            echo "Succ√®s : M√©trique success envoy√©e, code HTTP $STATUS"
          else
            echo "Erreur : √âchec de l'envoi de la m√©trique success, code HTTP $STATUS"
            exit 1
          fi
        shell: bash
      - name: Check metrics in PushGateway
        run: |
          echo "üîç V√©rification des m√©triques dans PushGateway..."
          curl -s -L $PUSHGATEWAY_URL/metrics | grep -E 'scale|loss|mae|mse|aic|bic|llf|r2_score|rmse|df_model|df_resid|success|avg_reward|avg_cycles|avg_accuracy|total_reward|cycles|accuracy' || echo "‚ö†Ô∏è Pas de m√©triques visibles"
        shell: bash
      - name: Complete Monitoring
        run: |
          echo "‚úÖ Fin de l'ex√©cution, v√©rification via Grafana et Prometheus"
        shell: bash

  The error occurs in the integration-test job during the model.predict step of the PPO model. The traceback indicates a ValueError: Error: Unexpected observation shape () for Box environment, please use (3,) or (n_env, 3) for the observation shape. This suggests that the observation returned by env.reset()[0] from BESSBatteryEnv has an incorrect shape, which does not match the expected observation space of the environment (a Box with shape (3,)).

Additionally, there are warnings:

A warning about using an OpenAI Gym environment, recommending a transition to Gymnasium. This is informational and not the cause of the error, as shimmy is already installed to handle compatibility.
A warning about the PPO mini-batch size (batch_size=64) not being a factor of the rollout buffer size (n_steps * n_envs = 2). This can be optimized but is not the primary issue.
Root Cause
The BESSBatteryEnv environment‚Äôs reset method returns an observation with an empty shape (()) or an incompatible format, but the environment‚Äôs observation_space is defined as a Box expecting a shape of (3,).
This mismatch causes stable-baselines3 to fail when processing the observation in model.predict.
The issue likely stems from how BESSBatteryEnv processes the input CSV (test_lstm_predictions.csv) or defines its observation space.
Analyzing the Problem
Observation Shape: The error indicates that BESSBatteryEnv expects observations to be a 1D array of length 3 (shape (3,)), likely corresponding to features derived from the CSV columns (energyproduced, predicted_demand, demand).
CSV Content: The test CSV created in the workflow has three columns: energyproduced, predicted_demand, and demand. However, BESSBatteryEnv may not be extracting these correctly or may be returning a scalar or empty observation.
Environment Implementation: Without the source code of BESSBatteryEnv.py, we can‚Äôt confirm the exact issue, but it‚Äôs likely that:
The reset method returns an incorrect observation (e.g., a scalar or empty value).
The observation_space is defined as Box(shape=(3,)), but the actual observation doesn‚Äôt match.
Fixing the Issue
To resolve this, we need to ensure that BESSBatteryEnv returns an observation with the correct shape (3,). Since we can‚Äôt modify BESSBatteryEnv.py (it‚Äôs downloaded from a repository), we can:

Verify the CSV Format: Ensure the CSV has the exact columns and format expected by BESSBatteryEnv.
Debug the Observation: Add debugging to inspect the observation returned by env.reset() and the observation_space.
Adjust the Test: If the issue is specific to the test setup, we can modify the test to align with BESSBatteryEnv‚Äôs expectations.
Fix PPO Configuration: Address the mini-batch size warning by setting batch_size to a factor of n_steps * n_envs (e.g., 2).
Given the error, the most likely issue is with the observation shape. Let‚Äôs modify the integration test to:

Print the observation and observation_space for debugging.
Set batch_size=2 to resolve the mini-batch warning and avoid truncated batches.
Keep the CSV as is, assuming it matches the expected format.
Updated Workflow
Below is the corrected integration-test job. The build job remains unchanged, so I‚Äôll only show the integration-test job:

yaml

Copier
name: MLFlow and Snakemake Workflow

on:
  schedule:
    - cron: '0 8 * * *'  # Run daily at 8:00 UTC
  push:
    branches: [main]
  pull_request:
    branches: [main]

env:
  MLFLOW_URL: https://80f3-105-155-14-70.ngrok-free.app/
  PUSHGATEWAY_URL: https://c8f3-105-155-14-70.ngrok-free.app

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      # [Previous build job steps remain unchanged, as shown in prior responses]
      # ...

  integration-test:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'
      - name: Set up virtual environment and install dependencies
        run: |
          python -m venv venv
          source venv/bin/activate
          pip install --upgrade pip
          pip install pytest pytest-asyncio pytest-mock pandas numpy stable-baselines3 gym shimmy
        shell: bash
      - name: Download BESSBatteryEnv script
        run: |
          wget https://raw.githubusercontent.com/elmekadem-narjiss/BackUp_ML/refs/heads/main/Backend/BESSBatteryEnv.py -O BESSBatteryEnv.py
          ls -l BESSBatteryEnv.py
        shell: bash
      - name: Create test CSV for integration test
        run: |
          source venv/bin/activate
          python -c "
          import pandas as pd
          data = pd.DataFrame({
              'energyproduced': [100, 200, 300],
              'predicted_demand': [250, 350, 450],
              'demand': [240, 340, 440]
          })
          data.to_csv('test_lstm_predictions.csv', index=False)
          "
          ls -l test_lstm_predictions.csv
        shell: bash
      - name: Run integration test
        run: |
          source venv/bin/activate
          export PYTHONPATH=$PYTHONPATH:$GITHUB_WORKSPACE
          python -c "
          import numpy as np
          from stable_baselines3 import PPO
          from BESSBatteryEnv import BESSBatteryEnv
          env = BESSBatteryEnv('test_lstm_predictions.csv')
          print('Observation space:', env.observation_space)
          obs, info = env.reset()
          print('Observation from reset:', obs, 'Shape:', np.shape(obs))
          model = PPO('MlpPolicy', env, n_steps=2, batch_size=2, verbose=0)
          model.learn(total_timesteps=4)
          action, _ = model.predict(obs)
          print('Integration test passed: Model trained and predicted action:', action)
          "
        shell: bash
      - name: Upload test logs
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: integration-test-logs
          path: |
            *.log
