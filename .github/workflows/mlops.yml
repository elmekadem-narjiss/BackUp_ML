name: MLFlow and Snakemake Workflow

on:
  schedule:
    - cron: '0 8 * * *'  # Run daily at 8:00 UTC
  push:
    branches: [main]
  pull_request:
    branches: [main]

env:
  MLFLOW_URL: https://80f3-105-155-14-70.ngrok-free.app/
  PUSHGATEWAY_URL: https://c8f3-105-155-14-70.ngrok-free.app

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'
      - name: Set up virtual environment and install dependencies
        run: |
          python -m venv venv
          source venv/bin/activate
          pip install --upgrade pip
          pip install mlflow prometheus_client jq snakemake pulp==2.4 pulp[cbc] stable-baselines3 gym pandas numpy matplotlib shimmy jupyter nbconvert papermill google-auth google-auth-oauthlib google-api-python-client ipykernel pytest pytest-asyncio pytest-mock respx fastapi uvicorn tensorflow scikit-learn redis psycopg2-binary influxdb-client paho-mqtt statsmodels sqlalchemy httpx
          pip list | grep ipykernel || { echo "Erreur : ipykernel non install√©"; exit 1; }
          pip list | grep google-api-python-client || { echo "Erreur : google-api-python-client non install√©"; exit 1; }
        shell: bash
      - name: Install Jupyter kernel
        run: |
          source venv/bin/activate
          python -m ipykernel install --user --name python3 --display-name "Python 3 (venv)"
          echo "Kernel Jupyter install√© avec succ√®s."
        shell: bash
      - name: Save Google Drive token
        run: |
          echo '{"token": "ya29.a0AZYkNZigxH1DhQugv99ZweJfyZdWkklt1F9Ntn6JM5XFhf0q9Km2kecSEgHEgo1zNgKoRWJOUCkGjuGk96SHJKfTIEOMKEYWvzA1Ko3jTx1PR7i_SHwU8wgq-gZ29dwemUnKwgw2jxU0hp9U8jbFI9ZG6bAI9_3WHQ8VMt0xaCgYKAUQSARISFQHGX2Mi1HZSkTbbdf6FiXHFeKU_eA0175", "refresh_token": "1//09sy1DZcnY3jZCgYIARAAGAkSNwF-L9IrW4JaBOZSELGp4ClKWKCcDHRF0kD-Qpw2hOzkXtW-54-pNI29lU0rMPPI63IfdPtxYaE", "token_uri": "https://oauth2.googleapis.com/token", "client_id": "27688368522-s8f6r2og4ikhm9ngnnkgnvtkq01ahu4u.apps.googleusercontent.com", "client_secret": "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", "scopes": ["https://www.googleapis.com/auth/drive"], "universe_domain": "googleapis.com", "account": "", "expiry": "2025-05-03T23:38:04.625991Z"}' | jq '.' > token.json
          if [ -s token.json ]; then
            echo "token.json cr√©√© avec succ√®s."
            cat token.json | jq . || { echo "Erreur : token.json n'est pas un JSON valide"; exit 1; }
          else
            echo "Erreur : token.json est vide ou n'a pas pu √™tre cr√©√©."
            exit 1
          fi
        shell: bash
      - name: Download notebook and scripts
        run: |
          wget https://raw.githubusercontent.com/elmekadem-narjiss/BackUp_ML/refs/heads/main/Backend/ppo_pipeline.ipynb -O ppo_pipeline.ipynb
          wget https://raw.githubusercontent.com/elmekadem-narjiss/BackUp_ML/refs/heads/main/Backend/train_ppo.py -O train_ppo.py
          wget https://raw.githubusercontent.com/elmekadem-narjiss/BackUp_ML/refs/heads/main/Backend/evaluate_ppo.py -O evaluate_ppo.py
          wget https://raw.githubusercontent.com/elmekadem-narjiss/BackUp_ML/refs/heads/main/Backend/BESSBatteryEnv.py -O BESSBatteryEnv.py
        shell: bash
      - name: Download LSTM predictions from Google Drive
        run: |
          source venv/bin/activate
          python Backend/download_file.py || { echo "√âchec de l'√©tape Download LSTM predictions from Google Drive"; exit 1; }
          if [ -f lstm_predictions_charger.csv ]; then
            echo "Fichier lstm_predictions_charger.csv t√©l√©charg√© avec succ√®s."
            ls -l lstm_predictions_charger.csv
          else
            echo "Erreur : lstm_predictions_charger.csv n'a pas √©t√© cr√©√©."
            exit 1
          fi
        shell: bash
      - name: Execute notebook
        run: |
          source venv/bin/activate
          mkdir -p output
          python -m papermill ppo_pipeline.ipynb output/ppo_pipeline_executed.ipynb \
            -p MLFLOW_URL $MLFLOW_URL \
            -p PUSHGATEWAY_URL $PUSHGATEWAY_URL \
            -p output_dir output \
            -p file_path lstm_predictions_charger.csv \
            --kernel python3
          echo "V√©rification des fichiers JSON g√©n√©r√©s :"
          ls -l output/*.json || echo "Aucun fichier JSON trouv√© dans output/"
        shell: bash
      - name: Check PushGateway accessibility
        run: |
          echo "V√©rification de l'accessibilit√© de la PushGateway : $PUSHGATEWAY_URL"
          echo "Envoi de la m√©trique de test vers $PUSHGATEWAY_URL/metrics/job/mlflow_and_snakemake_metrics"
          STATUS=$(echo "test_accessibility{job=\"mlflow_and_snakemake_metrics\"} 1" | curl -s -L -w "%{http_code}" --data-binary @- $PUSHGATEWAY_URL/metrics/job/mlflow_and_snakemake_metrics -o /dev/null)
          if [ "$STATUS" -eq 200 ]; then
            echo "PushGateway accessible, code HTTP : $STATUS"
            echo "V√©rification de la m√©trique test_accessibility dans PushGateway :"
            curl -s -L $PUSHGATEWAY_URL/metrics | grep test_accessibility || echo "M√©trique test_accessibility non trouv√©e"
          else
            echo "Erreur : PushGateway renvoie le code HTTP $STATUS"
            echo "D√©bogage : En-t√™tes de la requ√™te POST :"
            curl -v -L -X POST $PUSHGATEWAY_URL/metrics/job/mlflow_and_snakemake_metrics 2>&1 || echo "√âchec de la requ√™te POST"
            echo "D√©bogage : En-t√™tes de la requ√™te GET pour l'endpoint metrics :"
            curl -v -L $PUSHGATEWAY_URL/metrics 2>&1 || echo "√âchec de la requ√™te GET"
            exit 1
          fi
        shell: bash
      - name: Push selected MLFlow metrics to Prometheus
        run: |
          source venv/bin/activate
          declare -A metrics_92f5=(
            [loss]=0.04144944250583649
            [mae]=0.1347774024638297
            [mse]=0.039917658308037784
            [r2_score]=0.6619863834818595
            [rmse]=0.19979403972100315
          )
          declare -A metrics_ad0c=(
            [aic]=3157.583271036795
            [bic]=3180.9434357641003
            [df_model]=5
            [df_resid]=Inf
            [llf]=-1573.7916355183975
            [mse]=0.005455112014267468
            [scale]=1
          )
          echo "Envoi des m√©triques metrics_92f5..."
          for key in "${!metrics_92f5[@]}"; do
            value=${metrics_92f5[$key]}
            echo "Envoi : $key{run=\"92f5893e1dbe4175a3f4313bc89c56b4\",job=\"mlflow_and_snakemake_metrics\"} $value"
            STATUS=$(echo "$key{run=\"92f5893e1dbe4175a3f4313bc89c56b4\",job=\"mlflow_and_snakemake_metrics\"} $value" | curl -s -L -w "%{http_code}" --data-binary @- $PUSHGATEWAY_URL/metrics/job/mlflow_and_snakemake_metrics -o /dev/null)
            if [ "$STATUS" -eq 200 ]; then
              echo "Succ√®s : M√©trique $key envoy√©e, code HTTP $STATUS"
            else
              echo "Erreur : √âchec de l'envoi de la m√©trique $key, code HTTP $STATUS"
              exit 1
            fi
          done
          echo "Envoi des m√©triques metrics_ad0c..."
          for key in "${!metrics_ad0c[@]}"; do
            value=${metrics_ad0c[$key]}
            echo "Envoi : $key{run=\"ad0cf78265204f34b84a40aa09895c7f\",job=\"mlflow_and_snakemake_metrics\"} $value"
            STATUS=$(echo "$key{run=\"ad0cf78265204f34b84a40aa09895c7f\",job=\"mlflow_and_snakemake_metrics\"} $value" | curl -s -L -w "%{http_code}" --data-binary @- $PUSHGATEWAY_URL/metrics/job/mlflow_and_snakemake_metrics -o /dev/null)
            if [ "$STATUS" -eq 200 ]; then
              echo "Succ√®s : M√©trique $key envoy√©e, code HTTP $STATUS"
            else
              echo "Erreur : √âchec de l'envoi de la m√©trique $key, code HTTP $STATUS"
              exit 1
            fi
          done
          echo "V√©rification des fichiers PPO JSON :"
          if [ -f output/ppo_bess_model_metrics.json ]; then
            echo "Fichier ppo_bess_model_metrics.json trouv√©. Contenu :"
            cat output/ppo_bess_model_metrics.json | jq . || echo "Erreur : ppo_bess_model_metrics.json n'est pas un JSON valide"
            TRAIN_METRICS=$(cat output/ppo_bess_model_metrics.json | jq -r 'to_entries[] | .key + "{run=\"ppo_training\",job=\"ppo_metrics\"} " + (.value | tostring)')
            if [ -z "$TRAIN_METRICS" ]; then
              echo "Erreur : Aucune m√©trique trouv√©e dans ppo_bess_model_metrics.json"
              exit 1
            else
              while IFS= read -r metric; do
                echo "Envoi de la m√©trique PPO (entra√Ænement) : $metric"
                STATUS=$(echo "$metric" | curl -s -L -w "%{http_code}" --data-binary @- $PUSHGATEWAY_URL/metrics/job/ppo_metrics -o /dev/null)
                if [ "$STATUS" -eq 200 ]; then
                  echo "Succ√®s : M√©trique PPO envoy√©e, code HTTP $STATUS"
                else
                  echo "Erreur : √âchec de l'envoi de la m√©trique PPO, code HTTP $STATUS"
                  exit 1
                fi
              done <<< "$TRAIN_METRICS"
            fi
          else
            echo "Erreur : ppo_bess_model_metrics.json non trouv√©. Aucune m√©trique PPO d'entra√Ænement envoy√©e."
            exit 1
          fi
          if [ -f output/evaluation_metrics.json ]; then
            echo "Fichier evaluation_metrics.json trouv√©. Contenu :"
            cat output/evaluation_metrics.json | jq . || echo "Erreur : evaluation_metrics.json n'est pas un JSON valide"
            EVAL_METRICS=$(cat output/evaluation_metrics.json | jq -r 'to_entries[] | .key + "{run=\"ppo_evaluation\",job=\"ppo_metrics\"} " + (.value | tostring)')
            if [ -z "$EVAL_METRICS" ]; then
              echo "Erreur : Aucune m√©trique trouv√©e dans evaluation_metrics.json"
              exit 1
            else
              while IFS= read -r metric; do
                echo "Envoi de la m√©trique PPO (√©valuation) : $metric"
                STATUS=$(echo "$metric" | curl -s -L -w "%{http_code}" --data-binary @- $PUSHGATEWAY_URL/metrics/job/ppo_metrics -o /dev/null)
                if [ "$STATUS" -eq 200 ]; then
                  echo "Succ√®s : M√©trique PPO envoy√©e, code HTTP $STATUS"
                else
                  echo "Erreur : √âchec de l'envoi de la m√©trique PPO, code HTTP $STATUS"
                  exit 1
                fi
              done <<< "$EVAL_METRICS"
            fi
          else
            echo "Erreur : evaluation_metrics.json non trouv√©. Aucune m√©trique PPO d'√©valuation envoy√©e."
            exit 1
          fi
        shell: bash
      - name: Push global success metric to Prometheus
        run: |
          STATUS=$(echo "success{job=\"mlflow_and_snakemake_metrics\"} 1" | curl -s -L -w "%{http_code}" --data-binary @- $PUSHGATEWAY_URL/metrics/job/mlflow_and_snakemake_metrics -o /dev/null)
          if [ "$STATUS" -eq 200 ]; then
            echo "Succ√®s : M√©trique success envoy√©e, code HTTP $STATUS"
          else
            echo "Erreur : √âchec de l'envoi de la m√©trique success, code HTTP $STATUS"
            exit 1
          fi
        shell: bash
      - name: Check metrics in PushGateway
        run: |
          echo "üîç V√©rification des m√©triques dans PushGateway..."
          curl -s -L $PUSHGATEWAY_URL/metrics | grep -E 'scale|loss|mae|mse|aic|bic|llf|r2_score|rmse|df_model|df_resid|success|avg_reward|avg_cycles|avg_accuracy|total_reward|cycles|accuracy' || echo "‚ö†Ô∏è Pas de m√©triques visibles"
        shell: bash
      - name: Complete Monitoring
        run: |
          echo "‚úÖ Fin de l'ex√©cution, v√©rification via Grafana et Prometheus"
        shell: bash

  test:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'
      - name: Set up virtual environment and install dependencies
        run: |
          python -m venv venv
          source venv/bin/activate
          pip install --upgrade pip
          pip install pytest pytest-asyncio pytest-mock respx numpy pandas stable-baselines3 gym fastapi uvicorn tensorflow scikit-learn redis psycopg2-binary influxdb-client paho-mqtt statsmodels sqlalchemy shimmy httpx
        shell: bash
      - name: Update test_main.py
        run: |
          cat << 'EOF' > Backend/tests/test_main.py
import pytest
from fastapi.testclient import TestClient
from app.main import app
from app.services.lstm_model import MODEL, SCALER, SEQ_LENGTH, PREDICTION_DAYS, load_model, save_predictions_to_db

client = TestClient(app)

@pytest.mark.asyncio
async def test_read_root():
    response = client.get("/")
    assert response.status_code == 200
    assert response.json() == {"message": "Welcome to the LSTM Prediction API"}

@pytest.mark.asyncio
async def test_predict_endpoint(mocker):
    # Mock load_model and save_predictions_to_db
    mocker.patch("app.services.lstm_model.load_model", return_value=MODEL)
    mocker.patch("app.services.lstm_model.save_predictions_to_db", return_value=None)
    
    # Sample input data
    input_data = {
        "data": [[100, 200, 300]] * SEQ_LENGTH,
        "days": PREDICTION_DAYS
    }
    
    response = client.post("/predict/", json=input_data)
    assert response.status_code == 200
    assert "predictions" in response.json()
EOF
        shell: bash
      - name: Update test_model_lstm.py
        run: |
          cat << 'EOF' > Backend/tests/test_model_lstm.py
import pytest
import numpy as np
from app.services.lstm_model import load_model, save_predictions_to_db

@pytest.mark.asyncio
async def test_load_model(mocker):
    mocker.patch("app.services.lstm_model.load_model", return_value=mocker.MagicMock())
    model = load_model()
    assert model is not None

@pytest.mark.asyncio
async def test_save_predictions_to_db(mocker):
    mocker.patch("app.services.lstm_model.save_predictions_to_db", return_value=None)
    predictions = np.array([1.0, 2.0, 3.0])
    save_predictions_to_db(predictions)
    assert True  # If no exception, test passes
EOF
        shell: bash
      - name: Update app/main.py
        run: |
          cat << 'EOF' > Backend/app/main.py
from fastapi import FastAPI
from pydantic import BaseModel
import numpy as np
from app.services.lstm_model import MODEL, SCALER, SEQ_LENGTH, PREDICTION_DAYS, load_model, save_predictions_to_db

app = FastAPI()

class PredictionInput(BaseModel):
    data: list
    days: int

@app.get("/")
async def read_root():
    return {"message": "Welcome to the LSTM Prediction API"}

@app.post("/predict/")
async def predict(input_data: PredictionInput):
    model = load_model()
    data = np.array(input_data.data)
    predictions = model.predict(data)  # Simplified; adjust based on actual model
    save_predictions_to_db(predictions)
    return {"predictions": predictions.tolist()}
EOF
        shell: bash
      - name: Ensure lstm_model.py exists
        run: |
          if [ ! -f Backend/app/services/lstm_model.py ]; then
            mkdir -p Backend/app/services
            cat << 'EOF' > Backend/app/services/lstm_model.py
from tensorflow.keras.models import Sequential
import numpy as np

MODEL = Sequential()  # Placeholder; replace with actual model
SCALER = None  # Placeholder; replace with actual scaler
SEQ_LENGTH = 10
PREDICTION_DAYS = 7

def load_model():
    return MODEL

def save_predictions_to_db(predictions):
    pass  # Implement database save logic if needed
EOF
            echo "Created Backend/app/services/lstm_model.py"
            cat Backend/app/services/lstm_model.py
          else
            echo "Backend/app/services/lstm_model.py already exists"
          fi
        shell: bash
      - name: Create test CSV for BESSBatteryEnv
        run: |
          source $GITHUB_WORKSPACE/venv/bin/activate
          cd Backend
          python -c "
import pandas as pd
data = pd.DataFrame({
    'energyproduced': [100, 200, 300],
    'predicted_demand': [250, 350, 450],
    'demand': [240, 340, 440]
})
data.to_csv('tests/test_lstm_predictions.csv', index=False)
"
          ls -l tests/test_lstm_predictions.csv
        shell: bash
      - name: Verify test CSV columns
        run: |
          source $GITHUB_WORKSPACE/venv/bin/activate
          cd Backend
          python -c "
import pandas as pd
df = pd.read_csv('tests/test_lstm_predictions.csv')
expected_columns = ['energyproduced', 'predicted_demand', 'demand']
assert all(col in df.columns for col in expected_columns), f'CSV missing expected columns: {expected_columns}, found: {list(df.columns)}'
print('Test CSV columns verified:', list(df.columns))
"
        shell: bash
      - name: Set up test environment
        run: |
          source $GITHUB_WORKSPACE/venv/bin/activate
          export PYTHONPATH=$PYTHONPATH:$GITHUB_WORKSPACE/Backend
          unset INFLUX_URL INFLUX_TOKEN INFLUX_ORG INFLUX_BUCKET
        shell: bash
      - name: Run tests
        run: |
          source $GITHUB_WORKSPACE/venv/bin/activate
          cd Backend
          pytest tests/ --verbose --log-file=pytest.log
        shell: bash
      - name: Upload test logs
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: test-logs
          path: |
            Backend/tests/*.log
            Backend/pytest.log
