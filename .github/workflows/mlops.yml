name: MLFlow and Snakemake Workflow

on:
  schedule:
    - cron: '0 8 * * *'  # Run daily at 8:00 UTC
  push:
    branches: [main]
  pull_request:
    branches: [main]

env:
  MLFLOW_URL: https://80f3-105-155-14-70.ngrok-free.app/
  PUSHGATEWAY_URL: https://c8f3-105-155-14-70.ngrok-free.app

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      # [Previous build job steps remain unchanged]
      - name: Checkout code
        uses: actions/checkout@v3
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'
      - name: Set up virtual environment and install dependencies
        run: |
          python -m venv venv
          source venv/bin/activate
          pip install --upgrade pip
          pip install mlflow prometheus_client jq snakemake pulp==2.4 pulp[cbc] stable-baselines3 gym pandas numpy matplotlib shimmy jupyter nbconvert papermill google-auth google-auth-oauthlib google-api-python-client ipykernel pytest pytest-asyncio pytest-mock respx fastapi uvicorn tensorflow scikit-learn redis psycopg2-binary influxdb-client paho-mqtt statsmodels sqlalchemy httpx
          pip list | grep ipykernel || { echo "Erreur : ipykernel non installé"; exit 1; }
          pip list | grep google-api-python-client || { echo "Erreur : google-api-python-client non installé"; exit 1; }
        shell: bash
      - name: Install Jupyter kernel
        run: |
          source venv/bin/activate
          python -m ipykernel install --user --name python3 --display-name "Python 3 (venv)"
          echo "Kernel Jupyter installé avec succès."
        shell: bash
      - name: Save Google Drive token
        run: |
          echo '{"token": "ya29.a0AZYkNZigxH1DhQugv99ZweJfyZdWkklt1F9Ntn6JM5XFhf0q9Km2kecSEgHEgo1zNgKoRWJOUCkGjuGk96SHJKfTIEOMKEYWvzA1Ko3jTx1PR7i_SHwU8wgq-gZ29dwemUnKwgw2jxU0hp9U8jbFI9ZG6bAI9_3WHQ8VMt0xaCgYKAUQSARISFQHGX2Mi1HZSkTbbdf6FiXHFeKU_eA0175", "refresh_token": "1//09sy1DZcnY3jZCgYIARAAGAkSNwF-L9IrW4JaBOZSELGp4ClKWKCcDHRF0kD-Qpw2hOzkXtW-54-pNI29lU0rMPPI63IfdPtxYaE", "token_uri": "https://oauth2.googleapis.com/token", "client_id": "27688368522-s8f6r2og4ikhm9ngnnkgnvtkq01ahu4u.apps.googleusercontent.com", "client_secret": "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", "scopes": ["https://www.googleapis.com/auth/drive"], "universe_domain": "googleapis.com", "account": "", "expiry": "2025-05-03T23:38:04.625991Z"}' | jq '.' > token.json
          if [ -s token.json ]; then
            echo "token.json créé avec succès."
            cat token.json | jq . || { echo "Erreur : token.json n'est pas un JSON valide"; exit 1; }
          else
            echo "Erreur : token.json est vide ou n'a pas pu être créé."
            exit 1
          fi
        shell: bash
      - name: Download notebook and scripts
        run: |
          wget https://raw.githubusercontent.com/elmekadem-narjiss/BackUp_ML/refs/heads/main/Backend/ppo_pipeline.ipynb -O ppo_pipeline.ipynb
          wget https://raw.githubusercontent.com/elmekadem-narjiss/BackUp_ML/refs/heads/main/Backend/train_ppo.py -O train_ppo.py
          wget https://raw.githubusercontent.com/elmekadem-narjiss/BackUp_ML/refs/heads/main/Backend/evaluate_ppo.py -O evaluate_ppo.py
          wget https://raw.githubusercontent.com/elmekadem-narjiss/BackUp_ML/refs/heads/main/Backend/BESSBatteryEnv.py -O BESSBatteryEnv.py
        shell: bash
      - name: Download LSTM predictions from Google Drive
        run: |
          source venv/bin/activate
          python Backend/download_file.py || { echo "Échec de l'étape Download LSTM predictions from Google Drive"; exit 1; }
          if [ -f lstm_predictions_charger.csv ]; then
            echo "Fichier lstm_predictions_charger.csv téléchargé avec succès."
            ls -l lstm_predictions_charger.csv
          else
            echo "Erreur : lstm_predictions_charger.csv n'a pas été créé."
            exit 1
          fi
        shell: bash
      - name: Execute notebook
        run: |
          source venv/bin/activate
          mkdir -p output
          python -m papermill ppo_pipeline.ipynb output/ppo_pipeline_executed.ipynb \
            -p MLFLOW_URL $MLFLOW_URL \
            -p PUSHGATEWAY_URL $PUSHGATEWAY_URL \
            -p output_dir output \
            -p file_path lstm_predictions_charger.csv \
            --kernel python3
          echo "Vérification des fichiers JSON générés :"
          ls -l output/*.json || echo "Aucun fichier JSON trouvé dans output/"
        shell: bash
      - name: Check PushGateway accessibility
        run: |
          echo "Vérification de l'accessibilité de la PushGateway : $PUSHGATEWAY_URL"
          echo "Envoi de la métrique de test vers $PUSHGATEWAY_URL/metrics/job/mlflow_and_snakemake_metrics"
          STATUS=$(echo "test_accessibility{job=\"mlflow_and_snakemake_metrics\"} 1" | curl -s -L -w "%{http_code}" --data-binary @- $PUSHGATEWAY_URL/metrics/job/mlflow_and_snakemake_metrics -o /dev/null)
          if [ "$STATUS" -eq 200 ]; then
            echo "PushGateway accessible, code HTTP : $STATUS"
            echo "Vérification de la métrique test_accessibility dans PushGateway :"
            curl -s -L $PUSHGATEWAY_URL/metrics | grep test_accessibility || echo "Métrique test_accessibility non trouvée"
          else
            echo "Erreur : PushGateway renvoie le code HTTP $STATUS"
            echo "Débogage : En-têtes de la requête POST :"
            curl -v -L -X POST $PUSHGATEWAY_URL/metrics/job/mlflow_and_snakemake_metrics 2>&1 || echo "Échec de la requête POST"
            echo "Débogage : En-têtes de la requête GET pour l'endpoint metrics :"
            curl -v -L $PUSHGATEWAY_URL/metrics 2>&1 || echo "Échec de la requête GET"
            exit 1
          fi
        shell: bash
      - name: Push selected MLFlow metrics to Prometheus
        run: |
          source venv/bin/activate
          declare -A metrics_92f5=(
            [loss]=0.04144944250583649
            [mae]=0.1347774024638297
            [mse]=0.039917658308037784
            [r2_score]=0.6619863834818595
            [rmse]=0.19979403972100315
          )
          declare -A metrics_ad0c=(
            [aic]=3157.583271036795
            [bic]=3180.9434357641003
            [df_model]=5
            [df_resid]=Inf
            [llf]=-1573.7916355183975
            [mse]=0.005455112014267468
            [scale]=1
          )
          echo "Envoi des métriques metrics_92f5..."
          for key in "${!metrics_92f5[@]}"; do
            value=${metrics_92f5[$key]}
            echo "Envoi : $key{run=\"92f5893e1dbe4175a3f4313bc89c56b4\",job=\"mlflow_and_snakemake_metrics\"} $value"
            STATUS=$(echo "$key{run=\"92f5893e1dbe4175a3f4313bc89c56b4\",job=\"mlflow_and_snakemake_metrics\"} $value" | curl -s -L -w "%{http_code}" --data-binary @- $PUSHGATEWAY_URL/metrics/job/mlflow_and_snakemake_metrics -o /dev/null)
            if [ "$STATUS" -eq 200 ]; then
              echo "Succès : Métrique $key envoyée, code HTTP $STATUS"
            else
              echo "Erreur : Échec de l'envoi de la métrique $key, code HTTP $STATUS"
              exit 1
            fi
          done
          echo "Envoi des métriques metrics_ad0c..."
          for key in "${!metrics_ad0c[@]}"; do
            value=${metrics_ad0c[$key]}
            echo "Envoi : $key{run=\"ad0cf78265204f34b84a40aa09895c7f\",job=\"mlflow_and_snakemake_metrics\"} $value"
            STATUS=$(echo "$key{run=\"ad0cf78265204f34b84a40aa09895c7f\",job=\"mlflow_and_snakemake_metrics\"} $value" | curl -s -L -w "%{http_code}" --data-binary @- $PUSHGATEWAY_URL/metrics/job/mlflow_and_snakemake_metrics -o /dev/null)
            if [ "$STATUS" -eq 200 ]; then
              echo "Succès : Métrique $key envoyée, code HTTP $STATUS"
            else
              echo "Erreur : Échec de l'envoi de la métrique $key, code HTTP $STATUS"
              exit 1
            fi
          done
          echo "Vérification des fichiers PPO JSON :"
          if [ -f output/ppo_bess_model_metrics.json ]; then
            echo "Fichier ppo_bess_model_metrics.json trouvé. Contenu :"
            cat output/ppo_bess_model_metrics.json | jq . || echo "Erreur : ppo_bess_model_metrics.json n'est pas un JSON valide"
            TRAIN_METRICS=$(cat output/ppo_bess_model_metrics.json | jq -r 'to_entries[] | .key + "{run=\"ppo_training\",job=\"ppo_metrics\"} " + (.value | tostring)')
            if [ -z "$TRAIN_METRICS" ]; then
              echo "Erreur : Aucune métrique trouvée dans ppo_bess_model_metrics.json"
              exit 1
            else
              while IFS= read -r metric; do
                echo "Envoi de la métrique PPO (entraînement) : $metric"
                STATUS=$(echo "$metric" | curl -s -L -w "%{http_code}" --data-binary @- $PUSHGATEWAY_URL/metrics/job/ppo_metrics -o /dev/null)
                if [ "$STATUS" -eq 200 ]; then
                  echo "Succès : Métrique PPO envoyée, code HTTP $STATUS"
                else
                  echo "Erreur : Échec de l'envoi de la métrique PPO, code HTTP $STATUS"
                  exit 1
                fi
              done <<< "$TRAIN_METRICS"
            fi
          else
            echo "Erreur : ppo_bess_model_metrics.json non trouvé. Aucune métrique PPO d'entraînement envoyée."
            exit 1
          fi
          if [ -f output/evaluation_metrics.json ]; then
            echo "Fichier evaluation_metrics.json trouvé. Contenu :"
            cat output/evaluation_metrics.json | jq . || echo "Erreur : evaluation_metrics.json n'est pas un JSON valide"
            EVAL_METRICS=$(cat output/evaluation_metrics.json | jq -r 'to_entries[] | .key + "{run=\"ppo_evaluation\",job=\"ppo_metrics\"} " + (.value | tostring)')
            if [ -z "$EVAL_METRICS" ]; then
              echo "Erreur : Aucune métrique trouvée dans evaluation_metrics.json"
              exit 1
            else
              while IFS= read -r metric; do
                echo "Envoi de la métrique PPO (évaluation) : $metric"
                STATUS=$(echo "$metric" | curl -s -L -w "%{http_code}" --data-binary @- $PUSHGATEWAY_URL/metrics/job/ppo_metrics -o /dev/null)
                if [ "$STATUS" -eq 200 ]; then
                  echo "Succès : Métrique PPO envoyée, code HTTP $STATUS"
                else
                  echo "Erreur : Échec de l'envoi de la métrique PPO, code HTTP $STATUS"
                  exit 1
                fi
              done <<< "$EVAL_METRICS"
            fi
          else
            echo "Erreur : evaluation_metrics.json non trouvé. Aucune métrique PPO d'évaluation envoyée."
            exit 1
          fi
        shell: bash
      - name: Push global success metric to Prometheus
        run: |
          STATUS=$(echo "success{job=\"mlflow_and_snakemake_metrics\"} 1" | curl -s -L -w "%{http_code}" --data-binary @- $PUSHGATEWAY_URL/metrics/job/mlflow_and_snakemake_metrics -o /dev/null)
          if [ "$STATUS" -eq 200 ]; then
            echo "Succès : Métrique success envoyée, code HTTP $STATUS"
          else
            echo "Erreur : Échec de l'envoi de la métrique success, code HTTP $STATUS"
            exit 1
          fi
        shell: bash
      - name: Check metrics in PushGateway
        run: |
          echo "🔍 Vérification des métriques dans PushGateway..."
          curl -s -L $PUSHGATEWAY_URL/metrics | grep -E 'scale|loss|mae|mse|aic|bic|llf|r2_score|rmse|df_model|df_resid|success|avg_reward|avg_cycles|avg_accuracy|total_reward|cycles|accuracy' || echo "⚠️ Pas de métriques visibles"
        shell: bash
      - name: Complete Monitoring
        run: |
          echo "✅ Fin de l'exécution, vérification via Grafana et Prometheus"
        shell: bash

  The error occurs in the integration-test job during the model.predict step of the PPO model. The traceback indicates a ValueError: Error: Unexpected observation shape () for Box environment, please use (3,) or (n_env, 3) for the observation shape. This suggests that the observation returned by env.reset()[0] from BESSBatteryEnv has an incorrect shape, which does not match the expected observation space of the environment (a Box with shape (3,)).

Additionally, there are warnings:

A warning about using an OpenAI Gym environment, recommending a transition to Gymnasium. This is informational and not the cause of the error, as shimmy is already installed to handle compatibility.
A warning about the PPO mini-batch size (batch_size=64) not being a factor of the rollout buffer size (n_steps * n_envs = 2). This can be optimized but is not the primary issue.
Root Cause
The BESSBatteryEnv environment’s reset method returns an observation with an empty shape (()) or an incompatible format, but the environment’s observation_space is defined as a Box expecting a shape of (3,).
This mismatch causes stable-baselines3 to fail when processing the observation in model.predict.
The issue likely stems from how BESSBatteryEnv processes the input CSV (test_lstm_predictions.csv) or defines its observation space.
Analyzing the Problem
Observation Shape: The error indicates that BESSBatteryEnv expects observations to be a 1D array of length 3 (shape (3,)), likely corresponding to features derived from the CSV columns (energyproduced, predicted_demand, demand).
CSV Content: The test CSV created in the workflow has three columns: energyproduced, predicted_demand, and demand. However, BESSBatteryEnv may not be extracting these correctly or may be returning a scalar or empty observation.
Environment Implementation: Without the source code of BESSBatteryEnv.py, we can’t confirm the exact issue, but it’s likely that:
The reset method returns an incorrect observation (e.g., a scalar or empty value).
The observation_space is defined as Box(shape=(3,)), but the actual observation doesn’t match.
Fixing the Issue
To resolve this, we need to ensure that BESSBatteryEnv returns an observation with the correct shape (3,). Since we can’t modify BESSBatteryEnv.py (it’s downloaded from a repository), we can:

Verify the CSV Format: Ensure the CSV has the exact columns and format expected by BESSBatteryEnv.
Debug the Observation: Add debugging to inspect the observation returned by env.reset() and the observation_space.
Adjust the Test: If the issue is specific to the test setup, we can modify the test to align with BESSBatteryEnv’s expectations.
Fix PPO Configuration: Address the mini-batch size warning by setting batch_size to a factor of n_steps * n_envs (e.g., 2).
Given the error, the most likely issue is with the observation shape. Let’s modify the integration test to:

Print the observation and observation_space for debugging.
Set batch_size=2 to resolve the mini-batch warning and avoid truncated batches.
Keep the CSV as is, assuming it matches the expected format.
Updated Workflow
Below is the corrected integration-test job. The build job remains unchanged, so I’ll only show the integration-test job:

yaml

Copier
name: MLFlow and Snakemake Workflow

on:
  schedule:
    - cron: '0 8 * * *'  # Run daily at 8:00 UTC
  push:
    branches: [main]
  pull_request:
    branches: [main]

env:
  MLFLOW_URL: https://80f3-105-155-14-70.ngrok-free.app/
  PUSHGATEWAY_URL: https://c8f3-105-155-14-70.ngrok-free.app

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      # [Previous build job steps remain unchanged, as shown in prior responses]
      # ...

  integration-test:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'
      - name: Set up virtual environment and install dependencies
        run: |
          python -m venv venv
          source venv/bin/activate
          pip install --upgrade pip
          pip install pytest pytest-asyncio pytest-mock pandas numpy stable-baselines3 gym shimmy
        shell: bash
      - name: Download BESSBatteryEnv script
        run: |
          wget https://raw.githubusercontent.com/elmekadem-narjiss/BackUp_ML/refs/heads/main/Backend/BESSBatteryEnv.py -O BESSBatteryEnv.py
          ls -l BESSBatteryEnv.py
        shell: bash
      - name: Create test CSV for integration test
        run: |
          source venv/bin/activate
          python -c "
          import pandas as pd
          data = pd.DataFrame({
              'energyproduced': [100, 200, 300],
              'predicted_demand': [250, 350, 450],
              'demand': [240, 340, 440]
          })
          data.to_csv('test_lstm_predictions.csv', index=False)
          "
          ls -l test_lstm_predictions.csv
        shell: bash
      - name: Run integration test
        run: |
          source venv/bin/activate
          export PYTHONPATH=$PYTHONPATH:$GITHUB_WORKSPACE
          python -c "
          import numpy as np
          from stable_baselines3 import PPO
          from BESSBatteryEnv import BESSBatteryEnv
          env = BESSBatteryEnv('test_lstm_predictions.csv')
          print('Observation space:', env.observation_space)
          obs, info = env.reset()
          print('Observation from reset:', obs, 'Shape:', np.shape(obs))
          model = PPO('MlpPolicy', env, n_steps=2, batch_size=2, verbose=0)
          model.learn(total_timesteps=4)
          action, _ = model.predict(obs)
          print('Integration test passed: Model trained and predicted action:', action)
          "
        shell: bash
      - name: Upload test logs
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: integration-test-logs
          path: |
            *.log
