name: MLFlow and Snakemake Workflow

on:
  schedule:
    - cron: '0 8 * * *'  # Run daily at 8:00 UTC
  push:
    branches: [main]
  pull_request:
    branches: [main]

env:
  MLFLOW_URL: https://c7db-41-142-107-118.ngrok-free.app/
  PUSHGATEWAY_URL: https://116a-41-142-107-118.ngrok-free.app

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'

      - name: Cache Python dependencies
        uses: actions/cache@v4
        with:
          path: venv
          key: ${{ runner.os }}-venv-${{ hashFiles('Backend/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-venv-

      - name: Verify requirements.txt
        run: |
          if [ ! -f Backend/requirements.txt ]; then
            echo "Error: Backend/requirements.txt not found"
            ls -R
            exit 1
          fi
          echo "Backend/requirements.txt found"
          cat Backend/requirements.txt
        shell: bash

      - name: Set up virtual environment and install dependencies
        run: |
          python -m venv venv
          source venv/bin/activate
          pip install --upgrade pip
          pip install -r Backend/requirements.txt
          pip install snakemake papermill google-api-python-client
          pip list | grep ipykernel || { echo "Erreur : ipykernel non installé"; exit 1; }
          pip list | grep google-api-python-client || { echo "Erreur : google-api-python-client non installé"; exit 1; }
          pip list | grep snakemake || { echo "Erreur : snakemake non installé"; exit 1; }
        shell: bash

      - name: Install Jupyter kernel
        run: |
          source venv/bin/activate
          python -m ipykernel install --user --name python3 --display-name "Python 3 (venv)"
          echo "Kernel Jupyter installé avec succès."
        shell: bash

      - name: Save Google Drive tokens
        run: |
          cat << 'EOF' > token.json
          {"token": "ya29.a0AZYkNZigxH1DhQugv99ZweJfyZdWkklt1F9Ntn6JM5XFhf0q9Km2kecSEgHEgo1zNgKoRWJOUCkGjuGk96SHJKfTIEOMKEYWvzA1Ko3jTx1PR7i_SHwU8wgq-gZ29dwemUnKwgw2jxU0hp9U8jbFI9ZG6bAI9_3WHQ8VMt0xaCgYKAUQSARISFQHGX2Mi1HZSkTbbdf6FiXHFeKU_eA0175", "refresh_token": "1//09sy1DZcnY3jZCgYIARAAGAkSNwF-L9IrW4JaBOZSELGp4ClKWKCcDHRF0kD-Qpw2hOzkXtW-54-pNI29lU0rMPPI63IfdPtxYaE", "token_uri": "https://oauth2.googleapis.com/token", "client_id": "27688368522-s8f6r2og4ikhm9ngnnkgnvtkq01ahu4u.apps.googleusercontent.com", "client_secret": "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", "scopes": ["https://www.googleapis.com/auth/drive"], "universe_domain": "googleapis.com", "account": "", "expiry": "2025-05-03T23:38:04.625991Z"}
          EOF
          cat << 'EOF' > client_secrets.json
          {"installed":{"client_id":"27688368522-s6f6r2og4ikhm9ngnnkgnvtkq01ahu4u.apps.googleusercontent.com","project_id":"colabautomation-458721","auth_uri":"https://accounts.google.com/o/oauth2/auth","token_uri":"https://oauth2.googleapis.com/token","auth_provider_x509_cert_url":"https://www.googleapis.com/oauth2/v1/certs","client_secret":"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","redirect_uris":["http://localhost"]}}
          EOF
          cat token.json | jq . > /dev/null || { echo "Erreur : token.json n'est pas un JSON valide"; exit 1; }
          cat client_secrets.json | jq . > /dev/null || { echo "Erreur : client_secrets.json n'est pas un JSON valide"; exit 1; }
          if [ -s token.json ]; then
            echo "token.json créé avec succès (size: $(stat -c %s token.json) bytes)"
          else
            echo "Erreur : token.json est vide ou n'a pas pu être créé"
            exit 1
          fi
          if [ -s client_secrets.json ]; then
            echo "client_secrets.json créé avec succès (size: $(stat -c %s client_secrets.json) bytes)"
          else
            echo "Erreur : client_secrets.json est vide ou n'a pas pu être créé"
            exit 1
          fi
        shell: bash

      - name: Verify config.yaml
        run: |
          if [ ! -f Backend/config.yaml ]; then
            echo "Error: Backend/config.yaml not found"
            ls -R Backend/
            exit 1
          fi
          echo "Backend/config.yaml found"
          cat Backend/config.yaml
        shell: bash

      - name: Download notebook and scripts
        run: |
          wget https://raw.githubusercontent.com/elmekadem-narjiss/BackUp_ML/refs/heads/main/Backend/ppo_pipeline.ipynb -O ppo_pipeline.ipynb
          wget https://raw.githubusercontent.com/elmekadem-narjiss/BackUp_ML/refs/heads/main/Backend/train_ppo.py -O train_ppo.py
          wget https://raw.githubusercontent.com/elmekadem-narjiss/BackUp_ML/refs/heads/main/Backend/evaluate_ppo.py -O evaluate_ppo.py
          wget https://raw.githubusercontent.com/elmekadem-narjiss/BackUp_ML/refs/heads/main/Backend/BESSBatteryEnv.py -O BESSBatteryEnv.py
          wget https://raw.githubusercontent.com/elmekadem-narjiss/BackUp_ML/refs/heads/main/Backend/download_file.py -O download_file.py
          ls -l train_ppo.py || echo "Erreur : train_ppo.py non téléchargé"
        shell: bash

      - name: Run Snakemake pipeline
        run: |
          source venv/bin/activate
          snakemake --snakefile Backend/Snakefile --cores 1 --config mlflow_url=$MLFLOW_URL pushgateway_url=$PUSHGATEWAY_URL
        shell: bash

      - name: Check PushGateway accessibility
        run: |
          echo "Vérification de l'accessibilité de la PushGateway : $PUSHGATEWAY_URL"
          echo "Envoi de la métrique de test vers $PUSHGATEWAY_URL/metrics/job/mlflow_and_snakemake_metrics"
          STATUS=$(echo "test_accessibility{job=\"mlflow_and_snakemake_metrics\"} 1" | curl -s -L -w "%{http_code}" --data-binary @- $PUSHGATEWAY_URL/metrics/job/mlflow_and_snakemake_metrics -o /dev/null)
          if [ "$STATUS" -eq 200 ]; then
            echo "PushGateway accessible, code HTTP : $STATUS"
            echo "Vérification de la métrique test_accessibility dans PushGateway :"
            curl -s -L $PUSHGATEWAY_URL/metrics | grep test_accessibility || echo "Métrique test_accessibility non trouvée"
          else
            echo "Erreur : PushGateway renvoie le code HTTP $STATUS"
            exit 1
          fi
        shell: bash

      - name: Push selected MLFlow metrics to Prometheus
        run: |
          source venv/bin/activate
          declare -A metrics_92f5=(
            [loss]=0.04144944250583649
            [mae]=0.1347774024638297
            [mse]=0.039917658308037784
            [r2_score]=0.6619863834818595
            [rmse]=0.19979403972100315
          )
          declare -A metrics_ad0c=(
            [aic]=3157.583271036795
            [bic]=3180.9434357641003
            [df_model]=5
            [df_resid]=Inf
            [llf]=-1573.7916355183975
            [mse]=0.005455112014267468
            [scale]=1
          )
          echo "Envoi des métriques metrics_92f5..."
          for key in "${!metrics_92f5[@]}"; do
            value=${metrics_92f5[$key]}
            echo "Envoi : $key{run=\"92f5893e1dbe4175a3f4313bc89c56b4\",job=\"mlflow_and_snakemake_metrics\"} $value"
            STATUS=$(echo "$key{run=\"92f5893e1dbe4175a3f4313bc89c56b4\",job=\"mlflow_and_snakemake_metrics\"} $value" | curl -s -L -w "%{http_code}" --data-binary @- $PUSHGATEWAY_URL/metrics/job/mlflow_and_snakemake_metrics -o /dev/null)
            if [ "$STATUS" -eq 200 ]; then
              echo "Succès : Métrique $key envoyée, code HTTP $STATUS"
            else
              echo "Erreur : Échec de l'envoi de la métrique $key, code HTTP $STATUS"
              exit 1
            fi
          done
          echo "Envoi des métriques metrics_ad0c..."
          for key in "${!metrics_ad0c[@]}"; do
            value=${metrics_ad0c[$key]}
            echo "Envoi : $key{run=\"ad0cf78265204f34b84a40aa09895c7f\",job=\"mlflow_and_snakemake_metrics\"} $value"
            STATUS=$(echo "$key{run=\"ad0cf78265204f34b84a40aa09895c7f\",job=\"mlflow_and_snakemake_metrics\"} $value" | curl -s -L -w "%{http_code}" --data-binary @- $PUSHGATEWAY_URL/metrics/job/mlflow_and_snakemake_metrics -o /dev/null)
            if [ "$STATUS" -eq 200 ]; then
              echo "Succès : Métrique $key envoyée, code HTTP $STATUS"
            else
              echo "Erreur : Échec de l'envoi de la métrique $key, code HTTP $STATUS"
              exit 1
            fi
          done
          echo "Vérification des fichiers PPO JSON :"
          if [ -f output/ppo_bess_model_metrics.json ]; then
            echo "Fichier ppo_bess_model_metrics.json trouvé. Contenu :"
            cat output/ppo_bess_model_metrics.json | jq . || echo "Erreur : ppo_bess_model_metrics.json n'est pas un JSON valide"
            TRAIN_METRICS=$(cat output/ppo_bess_model_metrics.json | jq -r 'to_entries[] | .key + "{run=\"ppo_training\",job=\"ppo_metrics\"} " + (.value | tostring)')
            if [ -z "$TRAIN_METRICS" ]; then
              echo "Avertissement : Aucune métrique trouvée dans ppo_bess_model_metrics.json, passage à l'étape suivante"
            else
              while IFS= read -r metric; do
                echo "Envoi de la métrique PPO (entraînement) : $metric"
                STATUS=$(echo "$metric" | curl -s -L -w "%{http_code}" --data-binary @- $PUSHGATEWAY_URL/metrics/job/ppo_metrics -o /dev/null)
                if [ "$STATUS" -eq 200 ]; then
                  echo "Succès : Métrique PPO envoyée, code HTTP $STATUS"
                else
                  echo "Erreur : Échec de l'envoi de la métrique PPO, code HTTP $STATUS"
                  exit 1
                fi
              done <<< "$TRAIN_METRICS"
            fi
          else
            echo "Avertissement : ppo_bess_model_metrics.json non trouvé. Passage à l'étape suivante sans métriques PPO d'entraînement."
          fi
          if [ -f output/evaluation_metrics.json ]; then
            echo "Fichier evaluation_metrics.json trouvé. Contenu :"
            cat output/evaluation_metrics.json | jq . || echo "Erreur : evaluation_metrics.json n'est pas un JSON valide"
            EVAL_METRICS=$(cat output/evaluation_metrics.json | jq -r 'to_entries[] | .key + "{run=\"ppo_evaluation\",job=\"ppo_metrics\"} " + (.value | tostring)')
            if [ -z "$EVAL_METRICS" ]; then
              echo "Avertissement : Aucune métrique trouvée dans evaluation_metrics.json, passage à l'étape suivante"
            else
              while IFS= read -r metric; do
                echo "Envoi de la métrique PPO (évaluation) : $metric"
                STATUS=$(echo "$metric" | curl -s -L -w "%{http_code}" --data-binary @- $PUSHGATEWAY_URL/metrics/job/ppo_metrics -o /dev/null)
                if [ "$STATUS" -eq 200 ]; then
                  echo "Succès : Métrique PPO envoyée, code HTTP $STATUS"
                else
                  echo "Erreur : Échec de l'envoi de la métrique PPO, code HTTP $STATUS"
                  exit 1
                fi
              done <<< "$EVAL_METRICS"
            fi
          else
            echo "Avertissement : evaluation_metrics.json non trouvé. Passage à l'étape suivante sans métriques PPO d'évaluation."
          fi
        shell: bash

      - name: Push global success metric to Prometheus
        run: |
          STATUS=$(echo "success{job=\"mlflow_and_snakemake_metrics\"} 1" | curl -s -L -w "%{http_code}" --data-binary @- $PUSHGATEWAY_URL/metrics/job/mlflow_and_snakemake_metrics -o /dev/null)
          if [ "$STATUS" -eq 200 ]; then
            echo "Succès : Métrique success envoyée, code HTTP $STATUS"
          else
            echo "Erreur : Échec de l'envoi de la métrique success, code HTTP $STATUS"
            exit 1
          fi
        shell: bash

      - name: Check metrics in PushGateway
        run: |
          echo "🔍 Vérification des métriques dans PushGateway..."
          curl -s -L $PUSHGATEWAY_URL/metrics | grep -E 'scale|loss|mae|mse|aic|bic|llf|r2_score|rmse|df_model|df_resid|success|avg_reward|avg_cycles|avg_accuracy|total_reward|cycles|accuracy' || echo "⚠️ Pas de métriques visibles"
        shell: bash

      - name: Complete Monitoring
        run: |
          echo "✅ Fin de l'exécution, vérification via Grafana et Prometheus"
        shell: bash

      - name: Upload build artifacts
        uses: actions/upload-artifact@v4
        with:
          name: build-artifacts
          path: |
            output/ppo_bess_model_metrics.json
            output/evaluation_metrics.json
            output/ppo_pipeline_executed.ipynb

      - name: Notify on build results
        if: always()
        run: |
          curl -X POST -H 'Content-type: application/json' \
          --data "{\"text\": \"Build job for ${{ github.repository }} completed.\nStatus: ${{ job.status }}\nRun: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}\"}" \
          ${{ secrets.SLACK_WEBHOOK_URL }}
        shell: bash

      - name: Record build workflow metrics
        run: |
          START_TIME=$(date +%s)
          # Simulate build execution (already done in previous steps)
          END_TIME=$(date +%s)
          DURATION=$((END_TIME - START_TIME))
          echo "workflow_duration{job=\"build\",repository=\"${{ github.repository }}\"} $DURATION" | curl -s -L --data-binary @- $PUSHGATEWAY_URL/metrics/job/workflow_metrics
        shell: bash

  integration-test:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'

      - name: Cache Python dependencies
        uses: actions/cache@v4
        with:
          path: venv
          key: ${{ runner.os }}-venv-${{ hashFiles('Backend/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-venv-

      - name: Verify requirements.txt
        run: |
          if [ ! -f Backend/requirements.txt ]; then
            echo "Error: Backend/requirements.txt not found"
            ls -R
            exit 1
          fi
          echo "Backend/requirements.txt found"
          cat Backend/requirements.txt
        shell: bash

      - name: Set up virtual environment and install dependencies
        run: |
          python -m venv venv
          source venv/bin/activate
          pip install --upgrade pip
          pip install -r Backend/requirements.txt
        shell: bash

      - name: Check tests directory structure
        run: |
          echo "Checking directory structure for tests..."
          ls -R Backend/tests || echo "Backend/tests does not exist"
          ls -R tests || echo "Root tests directory does not exist"
        shell: bash

      - name: Lint and format code
        run: |
          source venv/bin/activate
          # Skip black formatting check for integration testing
          echo "Skipping black formatting check to avoid reformatting files"
          # Lint with relaxed rules for integration testing
          if [ -d tests ] && [ "$(ls -A tests)" ]; then
            echo "Linting Backend and root tests directories with relaxed rules"
            flake8 Backend tests --max-line-length=88 --extend-ignore=E203,E501,F841,F401,E402,W291,E302,W293,E305,E231,E265,W292,E303 --exclude venv,Backend/tests
          else
            echo "Warning: root tests directory is empty or missing, linting only Backend with relaxed rules"
            flake8 Backend --max-line-length=88 --extend-ignore=E203,E501,F841,F401,E402,W291,E302,W293,E305,E231,E265,W292,E303 --exclude venv,Backend/tests
          fi
        shell: bash

      - name: Download BESSBatteryEnv script
        run: |
          wget https://raw.githubusercontent.com/elmekadem-narjiss/BackUp_ML/refs/heads/main/Backend/BESSBatteryEnv.py -O BESSBatteryEnv.py
          if [[ ! -s BESSBatteryEnv.py ]]; then
            echo "Error: BESSBatteryEnv.py is missing or empty"
            exit 1
          fi
          ls -l BESSBatteryEnv.py
        shell: bash

      - name: Create and validate test CSV
        run: |
          source venv/bin/activate
          python -c "
          import pandas as pd
          data = pd.DataFrame({
              'energyproduced': [100, 200, 300],
              'future_production': [250, 350, 450]
          })
          data.to_csv('test_lstm_predictions.csv', index=False)
          df = pd.read_csv('test_lstm_predictions.csv')
          required_columns = ['energyproduced', 'future_production']
          if not all(col in df.columns for col in required_columns):
            print('Error: Missing required columns in CSV')
            exit(1)
          if df.empty:
            print('Error: CSV is empty')
            exit(1)
          print('CSV validated successfully')
          "
          ls -l test_lstm_predictions.csv
        shell: bash

      - name: Run integration tests
        if: runner.os == 'Linux' && hashFiles('tests/*') != ''
        run: |
          source venv/bin/activate
          export PYTHONPATH=$PYTHONPATH:$GITHUB_WORKSPACE
          pytest tests/ --html=report.html --self-contained-html --junitxml=test-results.xml -v
        shell: bash

      - name: Upload test artifacts
        uses: actions/upload-artifact@v4
        with:
          name: test-artifacts
          path: |
            test-results.xml
            report.html
            *.log

      - name: Notify on test results
        if: always()
        run: |
          curl -X POST -H 'Content-type: application/json' \
          --data "{\"text\": \"Integration test for ${{ github.repository }} completed.\nStatus: ${{ job.status }}\nRun: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}\"}" \
          ${{ secrets.SLACK_WEBHOOK_URL }}
        shell: bash

      - name: Record workflow metrics
        run: |
          START_TIME=$(date +%s)
          # Simulate test execution (already done in previous step)
          END_TIME=$(date +%s)
          DURATION=$((END_TIME - START_TIME))
          echo "workflow_duration{job=\"integration_test\",repository=\"${{ github.repository }}\"} $DURATION" | curl -s -L --data-binary @- $PUSHGATEWAY_URL/metrics/job/workflow_metrics
        shell: bash

  deploy:
    runs-on: ubuntu-latest
    needs: build
    steps:
      - name: Checkout code
        uses: actions/checkout@v3
        with:
          fetch-depth: 0  # Fetch all history for proper git operations

      - name: Download build artifacts
        uses: actions/download-artifact@v4
        with:
          name: build-artifacts
          path: artifacts

      - name: Verify artifacts
        run: |
          ls -l artifacts/
          if [ -f artifacts/ppo_bess_model_metrics.json ] && [ -f artifacts/evaluation_metrics.json ] && [ -f artifacts/ppo_pipeline_executed.ipynb ]; then
            echo "Tous les artefacts sont présents."
          else
            echo "Erreur : Certains artefacts sont manquants."
            exit 1
          fi
        shell: bash

      - name: Prepare files for GitHub Pages
        run: |
          mkdir -p docs
          cp artifacts/ppo_bess_model_metrics.json docs/
          cp artifacts/evaluation_metrics.json docs/
          cp artifacts/ppo_pipeline_executed.ipynb docs/
          # Create a simple index.html to list the files
          cat << 'EOF' > docs/index.html
          <!DOCTYPE html>
          <html>
          <head>
            <title>MLFlow Pipeline Artifacts</title>
          </head>
          <body>
            <h1>Deployed Artifacts</h1>
            <ul>
              <li><a href="ppo_bess_model_metrics.json">PPO BESS Model Metrics</a></li>
              <li><a href="evaluation_metrics.json">Evaluation Metrics</a></li>
              <li><a href="ppo_pipeline_executed.ipynb">Executed PPO Pipeline Notebook</a></li>
            </ul>
          </body>
          </html>
          EOF
        shell: bash

      - name: Commit and push to main branch
        run: |
          git config --global user.name "GitHub Actions"
          git config --global user.email "actions@github.com"
          git add docs/
          git commit -m "Deploy artifacts to GitHub Pages (docs folder)" || echo "No changes to commit"
          git push https://x-access-token:${{ secrets.GITHUB_TOKEN }}@github.com/${{ github.repository }}.git main
        shell: bash

      - name: Notify on deploy results
        if: always()
        run: |
          DEPLOY_URL="https://${{ github.repository_owner }}.github.io/${{ github.repository_name }}/"
          curl -X POST -H 'Content-type: application/json' \
          --data "{\"text\": \"Deploy job for ${{ github.repository }} completed.\nStatus: ${{ job.status }}\nArtifacts deployed to: $DEPLOY_URL\nRun: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}\"}" \
          ${{ secrets.SLACK_WEBHOOK_URL }}
        shell: bash

      - name: Record deploy workflow metrics
        run: |
          START_TIME=$(date +%s)
          # Simulate deploy execution (already done in previous steps)
          END_TIME=$(date +%s)
          DURATION=$((END_TIME - START_TIME))
          echo "workflow_duration{job=\"deploy\",repository=\"${{ github.repository }}\"} $DURATION" | curl -s -L --data-binary @- $PUSHGATEWAY_URL/metrics/job/workflow_metrics
        shell: bash
