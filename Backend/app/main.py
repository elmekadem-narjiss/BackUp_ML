from fastapi import FastAPI, HTTPException
from pathlib import Path
from app.utils.time_series import load_energy_consumption_data ,save_data_to_influxdb
from app.services.prediction_service import apply_arima_model,save_predictions_to_postgres,get_influx_data,connect_postgresql
import pandas as pd
import numpy as np
from statsmodels.tsa.arima.model import ARIMA
from pydantic import BaseModel
import os
from app.services.enrich_data import add_time_features,load_data_from_postgres,save_to_influx,query_influx
from app.config.config import INFLUX_URL, INFLUX_ORG,INFLUX_TOKEN,INFLUX_BUCKET

from influxdb_client import InfluxDBClient
import logging
#from app.services.lstm_model import get_influxdb_client,load_data_from_influx,prepare_data_for_lstm,train_lstm


#from app.services.lstm_model import get_influxdb_client,load_data_from_influx,train_lstm,prepare_data_for_lstm


app = FastAPI()


# D√©finir le chemin du fichier de donn√©es
DATASET_DIR = Path("D:/PFE/DataSet")
#FILE_CSV = DATASET_DIR / "Energy_consumption.csv"
FILE_CSV = "D:/PFE/DataSet/Energy_consumption.csv"
data_cache = None  # Stocke les donn√©es apr√®s /load-data

@app.get("/")
def root():
    return {"message": "Bienvenue dans le service de pr√©diction"}

#charge donnee
@app.get("/load-data")
def load_data():
    global data_cache  # Permet de modifier la variable globale

    try:
        print("üìÇ V√©rification de l'existence du fichier CSV...")
        if not os.path.exists(str(FILE_CSV)):  # V√©rifier si le fichier existe
            raise HTTPException(status_code=404, detail="Fichier introuvable. V√©rifiez le chemin.")

        # Charger les donn√©es et r√©cup√©rer le nombre de lignes
        df, nombre_de_lignes = load_energy_consumption_data(str(FILE_CSV))

        print(f"‚úÖ {nombre_de_lignes} lignes charg√©es apr√®s nettoyage.")
        print("üîç V√©rification des premi√®res lignes du DataFrame...")
        print(df.head())

        # V√©rification des colonnes n√©cessaires
        if not all(col in df.columns for col in ['Year', 'Month', 'Day', 'Hour']):
            raise HTTPException(status_code=400, detail="Erreur : Les colonnes 'Year', 'Month', 'Day', 'Hour' sont manquantes.")

        # Reconstituer la colonne 'Timestamp'
        df['Timestamp'] = pd.to_datetime(df[['Year', 'Month', 'Day', 'Hour']], errors='coerce')

        if 'Timestamp' not in df.columns or df['Timestamp'].isnull().all():
            raise HTTPException(status_code=400, detail="Erreur : La colonne 'Timestamp' est absente ou contient des valeurs invalides.")

        # Enregistrer les donn√©es
        data_cache = df

        # Sauvegarder dans InfluxDB
        save_data_to_influxdb(df)

        # Retourner les donn√©es sous forme de dictionnaire avec le nombre de lignes
        data_dict = df[['Timestamp', 'Temperature', 'Humidity', 'SquareFootage', 'Occupancy', 'RenewableEnergy', 'EnergyConsumption']].head().to_dict(orient="records")
        return {"nombre_de_lignes": nombre_de_lignes, "data": data_dict}

    except Exception as e:
        print(f"‚ùå Exception captur√©e : {e}")
        raise HTTPException(status_code=500, detail=f"Erreur lors du chargement des donn√©es : {e}")
    

#charge pour sarimax
@app.get("/forecast")
def forecast_data():
    """
    Route pour effectuer des pr√©visions avec SARIMAX et les enregistrer dans PostgreSQL.
    """
    global data_cache  

    # V√©rification de la pr√©sence de donn√©es
    if data_cache is None:
        raise HTTPException(status_code=400, detail="Les donn√©es doivent d'abord √™tre charg√©es via /load-data")

    # Nettoyage des colonnes : enlever les espaces et convertir en minuscules
    data_cache.columns = data_cache.columns.str.strip().str.lower()
    logging.debug(f"Noms des colonnes apr√®s nettoyage : {list(data_cache.columns)}")

    # V√©rification des colonnes obligatoires
    required_columns = ["timestamp", "energyconsumption", "temperature", "humidity"]
    missing_columns = [col for col in required_columns if col not in data_cache.columns]

    if missing_columns:
        logging.error(f"Erreur : Colonnes manquantes {missing_columns}")
        raise HTTPException(status_code=400, detail=f"Colonnes manquantes : {missing_columns}")

    # Renommage de 'energyconsumption' en 'energyproduced'
    data_cache.rename(columns={"energyconsumption": "energyproduced"}, inplace=True)

    # Assurer que 'timestamp' est bien une colonne datetime
    data_cache["timestamp"] = pd.to_datetime(data_cache["timestamp"], errors="coerce")

    # V√©rifier les valeurs manquantes
    if data_cache["timestamp"].isnull().any():
        logging.error("Erreur : La colonne 'timestamp' contient des valeurs invalides.")
        raise HTTPException(status_code=400, detail="La colonne 'timestamp' contient des valeurs invalides.")

    if data_cache["energyproduced"].isnull().any():
        logging.error("Erreur : La colonne 'energyproduced' contient des valeurs manquantes.")
        raise HTTPException(status_code=400, detail="La colonne 'energyproduced' contient des valeurs manquantes.")

    try:
        num_data_points = len(data_cache)
        logging.debug(f"Nombre de points de donn√©es disponibles : {num_data_points}")

        # Conversion de 'energyproduced' en num√©rique
        data_cache["energyproduced"] = pd.to_numeric(data_cache["energyproduced"], errors='coerce')
        data_cache.dropna(subset=["energyproduced"], inplace=True)

        # S√©lection des variables exog√®nes
        exog_variables = data_cache[['temperature', 'humidity']]

        if not pd.api.types.is_numeric_dtype(data_cache["energyproduced"]):
            raise HTTPException(status_code=400, detail="La colonne 'energyproduced' doit √™tre num√©rique.")

        # Application du mod√®le SARIMAX
        logging.info("D√©but de la g√©n√©ration des pr√©visions avec SARIMAX...")
        forecast_json = apply_arima_model(data_cache, steps=1000)

        # Log du nombre de pr√©dictions g√©n√©r√©es
        logging.info(f"Nombre de pr√©dictions g√©n√©r√©es : {forecast_json['nombre_de_lignes']}")

        # Sauvegarde des pr√©visions dans PostgreSQL
        save_predictions_to_postgres(forecast_json)

        return {
            "message": "Pr√©visions g√©n√©r√©es et enregistr√©es avec succ√®s.",
            "forecast": forecast_json
        }

    except Exception as e:
        logging.exception("Erreur lors de la g√©n√©ration des pr√©visions")
        raise HTTPException(status_code=500, detail=f"Erreur lors de la g√©n√©ration des pr√©visions : {e}")


#Enrichi
@app.get("/data")
async def get_data():
    """Endpoint pour r√©cup√©rer les donn√©es depuis InfluxDB et afficher toutes les √©tapes du processus ETL"""
    try:
        logging.info("D√©but du processus ETL pour r√©cup√©rer et afficher les donn√©es...")

        # Charger les donn√©es depuis PostgreSQL
        df = load_data_from_postgres()
        row_count = len(df)

        if df.empty:
            logging.info("Aucune donn√©e disponible dans PostgreSQL.")
            return {"message": "Aucune donn√©e disponible dans PostgreSQL.", "count_postgres": 0}

        logging.info(f"{row_count} lignes charg√©es depuis PostgreSQL avec succ√®s.")

        # Ajouter les variables temporelles
        df = add_time_features(df)
        logging.info("Variables temporelles ajout√©es aux donn√©es.")

        # Sauvegarder les donn√©es dans InfluxDB
        save_to_influx(df)
        logging.info("Donn√©es sauvegard√©es dans InfluxDB.")

        # R√©cup√©rer les donn√©es enrichies depuis InfluxDB
        data = query_influx()

        # Retourner les donn√©es r√©cup√©r√©es depuis InfluxDB
        return {
            "data": data,
            "count_postgres": row_count,
            "count_influx": len(data)
        }

    except Exception as e:
        logging.error(f"Erreur critique dans le processus ETL : {e}")
        return {"error": f"Une erreur s'est produite lors du processus ETL : {e}"}


class PredictionRequest(BaseModel):
    steps: int = 30

@app.post("/predict")
async def predict(request: PredictionRequest):
    """Endpoint pour r√©cup√©rer les donn√©es, faire une pr√©diction et enregistrer les r√©sultats."""
    
    # √âtape 1 : Charger les donn√©es depuis InfluxDB
    data = get_influx_data()
    
    print("Donn√©es r√©cup√©r√©es :", data)  # Ajout pour voir les donn√©es
    
    # V√©rification
    if "energyConsumption" not in data.columns:
        raise ValueError("Les donn√©es r√©cup√©r√©es ne contiennent pas 'energyConsumption'.")

    # √âtape 2 : Appliquer le mod√®le ARIMA pour pr√©dire
    forecast_df = apply_arima_model(data, steps=request.steps)

    # √âtape 3 : Enregistrer les pr√©visions dans PostgreSQL
    save_predictions_to_postgres(forecast_df)

    return {"message": "Les pr√©visions ont √©t√© g√©n√©r√©es et enregistr√©es avec succ√®s."}





#Vlaue in  Postgres 
@app.get("/predictions")
async def get_predictions():
    """Endpoint pour r√©cup√©rer les pr√©visions enregistr√©es dans PostgreSQL"""
    conn = connect_postgresql()
    cursor = conn.cursor()
    
    cursor.execute("SELECT timestamp, energyproduced FROM predictions ORDER BY timestamp DESC")
    data = cursor.fetchall()

    cursor.close()
    conn.close()

    if not data:
        return {"message": "Aucune pr√©vision disponible."}

    predictions = [{"timestamp": row[0], "energyproduced": row[1]} for row in data]
    return {"predictions": predictions}




@app.get("/sync-postgres-to-influx")
def sync_postgres_to_influx():
    """
    Endpoint pour synchroniser les donn√©es de PostgreSQL vers InfluxDB.
    """
    try:
        # Charger les donn√©es depuis PostgreSQL
        df = load_data_from_postgres()

        if df.empty:
            raise HTTPException(status_code=404, detail="Aucune donn√©e trouv√©e dans PostgreSQL.")

        # Ajouter les variables temporelles
        df = add_time_features(df)

        # Afficher le nombre total de lignes apr√®s l'ajout des variables temporelles
        logging.debug(f"Nombre total de lignes apr√®s ajout des variables temporelles : {len(df)}")

        # Sauvegarder les donn√©es dans InfluxDB
        save_to_influx(df)

        # Retourner un aper√ßu des nouvelles donn√©es
        return {
            "message": "Donn√©es synchronis√©es avec succ√®s.",
            "preview": df.head().to_dict(orient="records"),
            "total_rows": len(df)  # Ajouter le nombre total de lignes trait√©es
        }

    except Exception as e:
        logging.error(f"Erreur lors de la synchronisation : {e}")
        raise HTTPException(status_code=500, detail=f"Erreur lors de la synchronisation : {e}")


@app.get("/get-influx-data")
def get_influx_data():
    """
    Endpoint pour r√©cup√©rer les donn√©es enregistr√©es dans InfluxDB et compter le nombre de lignes.
    """
    try:
        client = InfluxDBClient(url=INFLUX_URL, token=INFLUX_TOKEN, org=INFLUX_ORG)
        query_api = client.query_api()

        query = f'''
        from(bucket: "{INFLUX_BUCKET}")
          |> range(start: 0)
          |> filter(fn: (r) => r._measurement == "environment_data")
          |> filter(fn: (r) => r._field == "energyConsumption")
        '''
        tables = query_api.query(query, org=INFLUX_ORG)

        results = []
        for table in tables:
            for record in table.records:
                results.append({
                    "timestamp": record.get_time(),
                    "forecast": record.get_value(),
                    "field": record.get_field()
                })

        client.close()

        nombre_de_lignes = len(results)  # ‚úÖ Nombre total de lignes r√©cup√©r√©es

        if not results:
            return {"message": "Aucune donn√©e trouv√©e dans InfluxDB.", "nombre_de_lignes": 0}

        return {"nombre_de_lignes": nombre_de_lignes, "data": results}

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erreur lors de la r√©cup√©ration des donn√©es InfluxDB : {e}")





from sklearn.preprocessing import MinMaxScaler
from fastapi import HTTPException
from fastapi.responses import JSONResponse
import numpy as np
from app.services.lstm_model import MODEL,load_data_from_influx,SCALER,SEQ_LENGTH,PREDICTION_DAYS,load_model,save_predictions_to_db
import logging
import json
#from app.services.lstm_model import MODEL, load_data_from_influx, SCALER, SEQ_LENGTH, PREDICTION_DAYS, load_model

# Charger le mod√®le et le scaler sauvegard√©s
MODEL, SCALER = load_model()

FEATURE_NAMES = ['energyproduced', 'temperature', 'humidity', 'month', 'week_of_year', 'hour']

# D√©finir la classe pour g√©rer les types NumPy lors de la s√©rialisation en JSON
class NpEncoder(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        return super(NpEncoder, self).default(obj)

# Fonction pour charger les donn√©es depuis la base de donn√©es (InfluxDB)
df = load_data_from_influx()

def load_data_from_influx():
    # Ajouter un print ou un log pour v√©rifier les donn√©es r√©cup√©r√©es
    try:
        # Exemple : Charger les donn√©es de InfluxDB
        # df = some_influx_query_function()
        
        if df is None:
            print("Aucune donn√©e r√©cup√©r√©e de InfluxDB.")
            return None
        else:
            print(f"Nombre de lignes r√©cup√©r√©es : {len(df)}")
            return df
    except Exception as e:
        print(f"Erreur lors de la r√©cup√©ration des donn√©es : {e}")
        return None

# Fonction pour charger le mod√®le et le scaler sauvegard√©s
def load_model():
    # Impl√©mentation pour charger le mod√®le et le scaler depuis un fichier
    pass

from datetime import timedelta


@app.get("/predict")
def predict_from_db():
    df = load_data_from_influx()

    # V√©rifier si df est None ou vide
    if df is None or len(df) == 0:
        raise HTTPException(status_code=400, detail="Donn√©es manquantes ou erreur de chargement des donn√©es.")

    if len(df) < SEQ_LENGTH:
        raise HTTPException(status_code=400, detail="Pas assez de donn√©es")

    df = df[FEATURE_NAMES]
    last_sequence = df[-SEQ_LENGTH:]

    # Normalisation des donn√©es
    scaled_input = SCALER.transform(last_sequence)
    scaled_input = np.expand_dims(scaled_input, axis=0)

    # Pr√©diction avec le mod√®le
    prediction = MODEL.predict(scaled_input)[0]  # shape: (PREDICTION_DAYS, 6)

    # Inversion du scaling pour retrouver les valeurs originales
    inverse_scaled = SCALER.inverse_transform(prediction)  # Pas besoin de dummy

    # Arrondir les colonnes discr√®tes et formater les pr√©dictions
    formatted_predictions = []
    for row in inverse_scaled:
        prediction_dict = dict(zip(FEATURE_NAMES, row))

        # Convertir explicitement les types NumPy vers des types Python natifs
        prediction_dict = {
            "energyproduced": float(prediction_dict["energyproduced"]),
            "temperature": float(prediction_dict["temperature"]),
            "humidity": float(prediction_dict["humidity"]),
            "month": int(round(prediction_dict["month"])),
            "week_of_year": int(round(prediction_dict["week_of_year"])),
            "hour": int(round(prediction_dict["hour"]))
        }

        formatted_predictions.append(prediction_dict)

    # Sauvegarder les pr√©dictions dans la base de donn√©es ou fichier
    save_predictions_to_db(formatted_predictions)  # Assurez-vous que cette fonction est d√©finie pour la sauvegarde

    # Retourner la r√©ponse JSON avec les pr√©dictions format√©es
    return JSONResponse(content=json.loads(json.dumps({"predictions": formatted_predictions}, cls=NpEncoder)))
